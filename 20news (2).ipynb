{"cells":[{"cell_type":"markdown","id":"aac43cc9","metadata":{"id":"aac43cc9"},"source":["# <h1 align='center'>  la classification automatique et la discrimination multi-classes. </h1>"]},{"cell_type":"markdown","id":"718918be","metadata":{"id":"718918be"},"source":["### OBJECTIF :"]},{"cell_type":"markdown","id":"66825551","metadata":{"id":"66825551"},"source":["\n","\n","L'objectif de la classification des newsgroups est de développer des modèles d'apprentissage automatique capables de prédire la classe thématique d'un document à partir de son contenu textuel. Cela peut être réalisé en utilisant diverses techniques d'apprentissage automatique, telles que les classificateurs basés sur les arbres de décision, les machines à vecteurs de support (SVM), les réseaux de neurones, etc."]},{"cell_type":"markdown","id":"a4670f45","metadata":{"id":"a4670f45"},"source":["### DATA SET DESCRIPTION :"]},{"cell_type":"markdown","id":"ef4e0411","metadata":{"id":"ef4e0411"},"source":["Le jeu de données **\"Twenty Newsgroups\"** est un ensemble de données couramment utilisé dans le domaine de l'apprentissage automatique pour la classification de texte. L'objectif principal de ce jeu de données est de permettre la classification automatique des articles de presse en fonction de leur thème, ainsi que la discrimination entre plusieurs classes.\n","\n","Les données du jeu de données \"Twenty Newsgroups\" sont disponibles à l'adresse suivante : http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups. Vous pouvez y trouver les fichiers nécessaires pour télécharger les données et les informations supplémentaires.\n","\n","Voici une description générale du jeu de données :\n","\n","- Le jeu de données comprend un total de 20 000 documents issus de 20 newsgroups différents, d'où le nom du jeu de données.\n","\n","- Les newsgroups couvrent une large gamme de sujets, notamment la politique, les sports, la religion, les ordinateurs, les sciences, etc.\n","\n","- Chaque document est un article de presse en texte brut, sans mise en forme supplémentaire.\n","\n","- Les documents sont déjà prétraités pour supprimer les en-têtes, les signatures et les citations, afin de se concentrer uniquement sur le contenu principal.\n","\n","- Les documents sont répartis de manière équilibrée entre les 20 classes, avec 1 000 documents par classe."]},{"cell_type":"markdown","id":"b6b9aba4","metadata":{"id":"b6b9aba4"},"source":["En utilisant ce jeu de données, nous pouvons construire et entraîner des modèles de classification pour prédire la classe thématique d'un article de presse inconnu. Il est également possible d'explorer les différences et les similitudes entre les classes et de réaliser des analyses supplémentaires sur les données textuelles."]},{"cell_type":"markdown","id":"d8b776bf","metadata":{"id":"d8b776bf"},"source":["### IMPORTATION DES BIBLIOTHEQUES :"]},{"cell_type":"code","source":["pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Z5OOV_l47Ep","executionInfo":{"status":"ok","timestamp":1686477507120,"user_tz":-60,"elapsed":43980,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"e57b4bb8-6460-475d-e53e-12a561f2feee"},"id":"-Z5OOV_l47Ep","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=9c8a4a99c738df057ced27738a3ba16fe014fe789ec13bfb19e212219a746ed0\n","  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.4.0\n"]}]},{"cell_type":"code","execution_count":null,"id":"efe8443a-a2ae-415a-a873-822fbb1ce93b","metadata":{"tags":[],"id":"efe8443a-a2ae-415a-a873-822fbb1ce93b"},"outputs":[],"source":["from pyspark import SparkContext\n","\n","from pyspark.sql.types import *\n","from pyspark.sql import Row\n","from pyspark.sql import SQLContext\n","\n","from pyspark.ml.linalg import Vector\n","from pyspark.ml import Pipeline, PipelineModel\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer"]},{"cell_type":"markdown","id":"bf5d6bd8","metadata":{"id":"bf5d6bd8"},"source":["### Download dataset file :"]},{"cell_type":"code","execution_count":null,"id":"fbe11fca-b5d3-484d-a4a8-72ce730ac371","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"fbe11fca-b5d3-484d-a4a8-72ce730ac371","executionInfo":{"status":"ok","timestamp":1686477540658,"user_tz":-60,"elapsed":989,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"23a1bdf0-dd4b-499a-e70d-e47c0e46536a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":4}],"source":["import sys\n","import os\n","\n","# URL of the file to be downloaded\n","url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/mini_newsgroups.tar.gz\"\n","\n","# Extract the filename from the URL\n","filename = os.path.basename(url)\n","\n","# Use the `wget` command to download the file\n","os.system(f\"wget {url}\")\n"]},{"cell_type":"markdown","id":"3d646bd3","metadata":{"id":"3d646bd3"},"source":["### Current working directory :"]},{"cell_type":"markdown","id":"eb24ffac","metadata":{"id":"eb24ffac"},"source":["The **os.getcwd()** function is used to retrieve the current working directory, which is the directory in which the Python script is being executed."]},{"cell_type":"code","execution_count":null,"id":"3a7db303-02b0-4162-b8e2-cd446bf57647","metadata":{"tags":[],"id":"3a7db303-02b0-4162-b8e2-cd446bf57647"},"outputs":[],"source":["import os\n","\n","\n","# Get the current working directory\n","current_directory = os.getcwd()"]},{"cell_type":"markdown","id":"91458428","metadata":{"id":"91458428"},"source":["### Extraction of tar.gz :"]},{"cell_type":"markdown","id":"3e159ca9","metadata":{"id":"3e159ca9"},"source":["The **subprocess.run()** function is used to execute the tar command with specific arguments to extract the contents of the tar.gz archive. \n","\n","The **-xzf** options are used to extract the archive, while **-C** is used to specify the target directory for extraction, which in this case is current_directory."]},{"cell_type":"code","execution_count":null,"id":"b2e75559-44b3-43a5-974f-1b53cbf53fff","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"b2e75559-44b3-43a5-974f-1b53cbf53fff","executionInfo":{"status":"ok","timestamp":1686477550553,"user_tz":-60,"elapsed":328,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"a385e312-574a-41eb-bd5a-ee39bd7b2d64"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["CompletedProcess(args=['tar', '-xzf', 'mini_newsgroups.tar.gz', '-C', '/content'], returncode=0)"]},"metadata":{},"execution_count":6}],"source":["import shlex, subprocess\n","# Replace '20news-bydate.tar.gz' with your file name\n","filename = 'mini_newsgroups.tar.gz'\n","\n","\n","\n","# Extract the tar.gz archive with specific directory\n","subprocess.run(['tar', '-xzf', filename, '-C',current_directory])"]},{"cell_type":"markdown","id":"073e5d61","metadata":{"id":"073e5d61"},"source":["### List directory contents :"]},{"cell_type":"markdown","id":"83d5505a","metadata":{"id":"83d5505a"},"source":["The **os.listdir()** function is used to obtain a list of all files and directories present in the specified directory_path. In this case, the directory_path variable is set to the path of the extracted directory, which is current_directory + \"/mini_newsgroups\"."]},{"cell_type":"code","execution_count":null,"id":"6787189d-f74f-4f88-8a0a-5370c50d20be","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"6787189d-f74f-4f88-8a0a-5370c50d20be","executionInfo":{"status":"ok","timestamp":1686477562082,"user_tz":-60,"elapsed":247,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"18091307-60dc-4961-f26e-3e129a657e30"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['misc.forsale',\n"," 'rec.motorcycles',\n"," 'comp.sys.mac.hardware',\n"," 'rec.autos',\n"," 'comp.sys.ibm.pc.hardware',\n"," 'comp.os.ms-windows.misc',\n"," 'sci.med',\n"," 'sci.crypt',\n"," 'alt.atheism',\n"," 'talk.politics.mideast',\n"," 'talk.religion.misc',\n"," 'soc.religion.christian',\n"," 'talk.politics.guns',\n"," 'sci.electronics',\n"," 'sci.space',\n"," 'talk.politics.misc',\n"," 'comp.graphics',\n"," 'rec.sport.hockey',\n"," 'rec.sport.baseball',\n"," 'comp.windows.x']"]},"metadata":{},"execution_count":7}],"source":["import os\n","\n","\n","# List the contents of the extracted directory\n","os.listdir(current_directory+\"/mini_newsgroups\")\n"]},{"cell_type":"markdown","id":"b67f5c60","metadata":{"id":"b67f5c60"},"source":["The output you provided shows an example list of contents within the extracted directory, including various subdirectories such as **'misc.forsale', 'alt.atheism', 'comp.sys.mac.hardware'**, etc."]},{"cell_type":"markdown","id":"f0bd64e0","metadata":{"id":"f0bd64e0"},"source":["### Create the path :"]},{"cell_type":"code","execution_count":null,"id":"d993867d-c536-40da-9e24-b2cb6d429c66","metadata":{"tags":[],"id":"d993867d-c536-40da-9e24-b2cb6d429c66"},"outputs":[],"source":["# Create the path to the files within the mini_newsgroups directory\n","\n","path=current_directory+\"/mini_newsgroups/*\""]},{"cell_type":"markdown","id":"7fe3e228","metadata":{"id":"7fe3e228"},"source":["By using this path variable, we can perform operations or access files and directories within the **\"mini_newsgroups\"** directory in the current working directory.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"e7663e35","metadata":{"id":"e7663e35"},"source":["### SparkSession, SparkContext, and SQLContext in PySpark :"]},{"cell_type":"markdown","id":"cda80fbc","metadata":{"id":"cda80fbc"},"source":["the creation of a **SparkSession**, **SparkContext**, and **SQLContext** in PySpark"]},{"cell_type":"code","execution_count":null,"id":"9409fc12-d8f2-4a8a-9a6b-4d9701d0828a","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"9409fc12-d8f2-4a8a-9a6b-4d9701d0828a","executionInfo":{"status":"ok","timestamp":1686477580090,"user_tz":-60,"elapsed":8543,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"7aed8ab0-4792-4689-9da3-f0004587fd17"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n","  warnings.warn(\n"]}],"source":["from pyspark.sql import SparkSession\n","from pyspark import SparkContext\n","from pyspark.sql import SQLContext\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"myApp\").getOrCreate()\n","\n","# Create a SparkContext\n","sc = spark.sparkContext\n","\n","# Create a SQLContext\n","sqlContext = SQLContext(sc)\n"]},{"cell_type":"markdown","id":"ba085c72","metadata":{"id":"ba085c72"},"source":["- The **SparkSession** is the entry point for programming Spark with the DataFrame and SQL APIs. It is responsible for coordinating the execution of tasks across the cluster. \n","\n","- The **SparkContext** represents the connection to a Spark cluster and is used to create RDDs (Resilient Distributed Datasets) and perform distributed computations.\n","\n","- The **SQLContext** is a class that enables the use of SQL and DataFrame API in PySpark. It provides a programming interface to work with structured and semi-structured data."]},{"cell_type":"markdown","id":"cd39a5a3","metadata":{"id":"cd39a5a3"},"source":["### Train_data :"]},{"cell_type":"markdown","id":"9393bfa9","metadata":{"id":"9393bfa9"},"source":["The **sc.wholeTextFiles()** method reads a collection of text files and returns an RDD (Resilient Distributed Dataset) where each element represents a file path and its corresponding content as a string. The path variable represents the path to the text files that you want to read."]},{"cell_type":"code","execution_count":null,"id":"8f26eaf0-8d20-46b2-986f-d22bae04b65f","metadata":{"tags":[],"id":"8f26eaf0-8d20-46b2-986f-d22bae04b65f"},"outputs":[],"source":["# Read the text files as a collection\n","train_data=sc.wholeTextFiles(path)"]},{"cell_type":"markdown","id":"ab090081","metadata":{"id":"ab090081"},"source":["### Usage of PySpark RDD :"]},{"cell_type":"markdown","id":"60be1e68","metadata":{"id":"60be1e68"},"source":["- The **map()** transformation is applied to the train_data RDD to extract the first element of each tuple (x[0]), which represents the file path.\n","\n","- The **takeSample()** action is used to randomly select a subset of file paths from the filepaths RDD."]},{"cell_type":"code","execution_count":null,"id":"d4d6e7dd-b5c9-4c46-b90d-004d4f26b561","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"d4d6e7dd-b5c9-4c46-b90d-004d4f26b561","executionInfo":{"status":"ok","timestamp":1686477693168,"user_tz":-60,"elapsed":15490,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"f0d96434-0aae-4386-f26e-0bf8fecda358"},"outputs":[{"output_type":"stream","name":"stdout","text":["['file:/content/mini_newsgroups/comp.sys.mac.hardware/52270', 'file:/content/mini_newsgroups/sci.crypt/15563', 'file:/content/mini_newsgroups/sci.space/61277', 'file:/content/mini_newsgroups/comp.sys.ibm.pc.hardware/60699', 'file:/content/mini_newsgroups/sci.electronics/54092']\n"]}],"source":["# Extract the file paths from the train_data RDD\n","filepaths = train_data.map(lambda x: x[0])\n","\n","# Sample a subset of file paths\n","print(filepaths.takeSample(False,5, 10))"]},{"cell_type":"markdown","id":"1cbc4d74","metadata":{"id":"1cbc4d74"},"source":["- **extract the text content and then sample a subset of it :** "]},{"cell_type":"code","execution_count":null,"id":"04c46cca-ef74-4c7b-be38-d1a4cc73eabd","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"04c46cca-ef74-4c7b-be38-d1a4cc73eabd","executionInfo":{"status":"ok","timestamp":1686477700735,"user_tz":-60,"elapsed":1810,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"ad523f17-4e5e-4bbc-eab4-b00fcbf4a63b"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Newsgroups: comp.sys.mac.hardware\\nPath: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!bb3.andrew.cmu.edu!news.sei.cmu.edu!cis.ohio-state.edu!zaphod.mps.ohio-state.edu!moe.ksu.ksu.edu!osuunx.ucc.okstate.edu!constellation!essex.ecn.uoknor.edu!cmparris\\nFrom: cmparris@essex.ecn.uoknor.edu (Chris Michael Parrish)\\nSubject: Networking Macs and a PC\\nSender: usenet@constellation.ecn.uoknor.edu (Usenet Administrator)\\nMessage-ID: <C5sHnJ.54L@constellation.ecn.uoknor.edu>\\nDate: Tue, 20 Apr 1993 15:57:11 GMT\\nNntp-Posting-Host: essex.ecn.uoknor.edu\\nOrganization: Engineering Computer Network, University of Oklahoma, Norman, OK, USA\\nLines: 24\\n\\n\\n  At work we have a small appletalk network with 3 macs and  couple of printers.\\nWe also have a PC that has some specialized accounting software that we would \\nlike to operate from any of the macs. We have Soft PC, and I have found that the\\nsoftware works just fine under it, but I would like to have all of the data\\nfor the program reside at one place (the PC hard disk). So my question for you\\nis(actually questions)\\n\\n 1) is there a board for the PC that will allow you to hook into an appletalk\\n    network?\\n\\n 2) if #1 is possible, is there any software/hardware combination that will \\n    allow me to mount the PC hard disk as a networked disk on the macs so I\\n    can use Soft PC to run the application?\\n\\n 3) if #1 or #2 is impossible, is there any other way to accomplish what I am\\n    after?\\n\\n\\n-- \\n_______________________________________________________________________________\\nChris Parrish                        |   \\nUniversity of Oklahoma               |    \"To share is to split...\"   \\ncmparris@essex.ecn.uoknor.edu        |        - KMFDM\\n']\n"]}],"source":["text = train_data.map(lambda x: x[1])\n","\n","print (text.takeSample(False,1, 10))"]},{"cell_type":"markdown","id":"2a4ffea4","metadata":{"id":"2a4ffea4"},"source":["The output shows an example of a subset containing one text content sampled from the text RDD. The text content represents a newsgroup message, including various header fields, the message body, and contact information."]},{"cell_type":"markdown","id":"ffd866f9","metadata":{"id":"ffd866f9"},"source":["- **extract the file IDs from the file paths :**"]},{"cell_type":"markdown","id":"86267ce9","metadata":{"id":"86267ce9"},"source":["The **map()** transformation is applied to the filepaths RDD to split each file path by the \"/\" delimiter and extract the last element (filepath.split(\"/\")[-1]), which represents the file ID. "]},{"cell_type":"code","execution_count":null,"id":"8efbf55b-0436-4deb-a01f-08dff0895ebf","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"8efbf55b-0436-4deb-a01f-08dff0895ebf","executionInfo":{"status":"ok","timestamp":1686477706988,"user_tz":-60,"elapsed":921,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"64a01af6-d4fb-4c94-d6f7-035dc4b33370"},"outputs":[{"output_type":"stream","name":"stdout","text":["['53759', '51186', '53336', '53399', '51227']\n"]}],"source":["id = filepaths.map(lambda filepath: filepath.split(\"/\")[-1])\n","print (id.take(5))"]},{"cell_type":"markdown","id":"d5a1bf57","metadata":{"id":"d5a1bf57"},"source":["The output shows an example of a subset of five file IDs sampled from the id RDD. Each file ID corresponds to a specific file in the \"mini_newsgroups\" directory."]},{"cell_type":"markdown","id":"1c53bada","metadata":{"id":"1c53bada"},"source":["- **extract the topics from the file paths :**"]},{"cell_type":"code","execution_count":null,"id":"0f69de82-eab9-486a-bcfa-dc44ad5949d6","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"0f69de82-eab9-486a-bcfa-dc44ad5949d6","executionInfo":{"status":"ok","timestamp":1686477712176,"user_tz":-60,"elapsed":641,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"65047e21-e3da-4982-ba69-d090ff77c9a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["['alt.atheism', 'alt.atheism', 'alt.atheism', 'alt.atheism', 'alt.atheism']\n"]}],"source":["# Extract the topics from the filepaths RDD\n","topics = filepaths.map(lambda filepath: filepath.split(\"/\")[-2])\n","\n","# Take a sample of the topics\n","print (topics.take(5))"]},{"cell_type":"markdown","id":"dcfe8efc","metadata":{"id":"dcfe8efc"},"source":["The output shows an example of a subset of five topics sampled from the topics RDD. Each topic corresponds to a specific category or theme within the \"mini_newsgroups\" dataset, such as 'alt.atheism', 'comp.sys.mac.hardware', etc."]},{"cell_type":"markdown","id":"9a0d2299","metadata":{"id":"9a0d2299"},"source":["- **extract distinct topics and take a sample of them :**"]},{"cell_type":"code","execution_count":null,"id":"85c78911-f857-48d0-880e-49c19c1146e5","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"85c78911-f857-48d0-880e-49c19c1146e5","executionInfo":{"status":"ok","timestamp":1686477721578,"user_tz":-60,"elapsed":3416,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"5fbdd1fc-e542-4733-cc4c-bb63cc9766d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["['alt.atheism', 'comp.os.ms-windows.misc', 'comp.windows.x', 'rec.motorcycles', 'sci.crypt', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'misc.forsale', 'rec.autos', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.electronics', 'sci.med', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"]}],"source":["# Take a sample of distinct topics\n","print( topics.distinct().take(20))"]},{"cell_type":"markdown","id":"1e2a01c4","metadata":{"id":"1e2a01c4"},"source":["The output shows an example of a subset of 20 distinct topics sampled from the distinct_topics RDD. Each topic represents a unique category or theme within the \"mini_newsgroups\" dataset, such as **'alt.atheism', 'comp.os.ms-windows.misc', 'comp.windows.x'**, etc."]},{"cell_type":"markdown","id":"92839dc0","metadata":{"id":"92839dc0"},"source":["### Create a DataFrame :"]},{"cell_type":"markdown","id":"0fd1936c","metadata":{"id":"0fd1936c"},"source":["the usage of PySpark DataFrame and SQL operations to create a DataFrame with a specified schema and register it as a temporary view."]},{"cell_type":"code","execution_count":null,"id":"de53704f-64cb-4e0a-bc66-4910d3c612f8","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"de53704f-64cb-4e0a-bc66-4910d3c612f8","executionInfo":{"status":"ok","timestamp":1686477729960,"user_tz":-60,"elapsed":4945,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"1723fec9-9fa2-4deb-94d4-9cef8f865479"},"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- id: string (nullable = true)\n"," |-- text: string (nullable = true)\n"," |-- topic: string (nullable = true)\n","\n"]}],"source":["from pyspark.sql.types import *\n","# The schema is encoded in a string.\n","schemaString = \"id text topic\"\n","fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n","schema = StructType(fields)\n","\n","# Apply the schema to the RDD.\n","newsgroups = train_data.map(lambda filepath_text: (filepath_text[0].split(\"/\")[-1], filepath_text[1], filepath_text[0].split(\"/\")[-2]))\n","df = sqlContext.createDataFrame(newsgroups, schema)\n","\n","#print schema\n","df.printSchema()\n","\n","# Creates a temporary view using the DataFrame\n","df.createOrReplaceTempView(\"newsgroups\")\n"]},{"cell_type":"markdown","id":"eb91e985","metadata":{"id":"eb91e985"},"source":["The output shows the printed schema of the df DataFrame, indicating the field names ('id', 'text', 'topic') and their data types (string)."]},{"cell_type":"markdown","id":"2d33ca79","metadata":{"id":"2d33ca79"},"source":[" - **Run SQL queries on a registered DataFrame :**"]},{"cell_type":"code","execution_count":null,"id":"82933c0b-34dc-44c4-8e77-28b3a130fb74","metadata":{"scrolled":true,"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"82933c0b-34dc-44c4-8e77-28b3a130fb74","executionInfo":{"status":"ok","timestamp":1686477738197,"user_tz":-60,"elapsed":1804,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"23a79b87-dbe5-4958-83af-9a75090c656e"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+-----------+--------------------+\n","|   id|      topic|                text|\n","+-----+-----------+--------------------+\n","|53759|alt.atheism|Newsgroups: alt.a...|\n","|51186|alt.atheism|Path: cantaloupe....|\n","|53336|alt.atheism|Xref: cantaloupe....|\n","|53399|alt.atheism|Path: cantaloupe....|\n","|51227|alt.atheism|Path: cantaloupe....|\n","+-----+-----------+--------------------+\n","\n"]}],"source":["# SQL can be run over DataFrames that have been registered as a table.\n","results = sqlContext.sql(\"SELECT id,topic,text FROM newsgroups limit 5\")\n","results.show()"]},{"cell_type":"markdown","id":"9292efce","metadata":{"id":"9292efce"},"source":["The output shows the result of the SQL query, which includes the 'id', 'topic', and 'text' columns for the first 5 rows of the DataFrame. Each row represents a record from the \"newsgroups\" table, containing the corresponding values for the selected columns."]},{"cell_type":"markdown","id":"0df8afac","metadata":{"id":"0df8afac"},"source":["- The usage of PySpark SQL to run a SQL query on the registered DataFrame \"newsgroups\" and obtain the distinct topics along with their respective counts."]},{"cell_type":"code","execution_count":null,"id":"27b42af8-902a-42a1-83d5-5337c57fadbc","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"27b42af8-902a-42a1-83d5-5337c57fadbc","executionInfo":{"status":"ok","timestamp":1686477753174,"user_tz":-60,"elapsed":2546,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"b0b5c7df-e871-445c-b428-332941ccebb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+---+\n","|               topic|cnt|\n","+--------------------+---+\n","|      comp.windows.x|100|\n","|        misc.forsale|100|\n","|    rec.sport.hockey|100|\n","|  rec.sport.baseball|100|\n","|comp.os.ms-window...|100|\n","+--------------------+---+\n","\n"]}],"source":["results = sqlContext.sql(\"select distinct topic, count(*) as cnt from newsgroups group by topic order by cnt desc limit 5\")\n","results.show()"]},{"cell_type":"markdown","id":"bbf9e00a","metadata":{"id":"bbf9e00a"},"source":["The output shows the result of the SQL query, which includes the distinct topics and their corresponding counts. The topics are sorted in descending order of counts, and the LIMIT clause restricts the result to the top 5 topics with the highest counts."]},{"cell_type":"markdown","id":"905c1a0a","metadata":{"id":"905c1a0a"},"source":["- **filters the DataFrame \"df\" :**"]},{"cell_type":"markdown","id":"1e2ffcc5","metadata":{"id":"1e2ffcc5"},"source":["Filters the DataFrame \"df\" based on the topic condition and creates a new DataFrame \"new_df\" from the filtered results. "]},{"cell_type":"code","execution_count":null,"id":"f859b680-5101-4bc3-9265-e79016bcb27e","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"f859b680-5101-4bc3-9265-e79016bcb27e","executionInfo":{"status":"ok","timestamp":1686477772804,"user_tz":-60,"elapsed":2583,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"bb0ba9ba-f01e-42c6-94cf-203ccd1ed2b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+--------------------+\n","|   id|                text|               topic|\n","+-----+--------------------+--------------------+\n","|38907|Path: cantaloupe....|       comp.graphics|\n","|38904|Xref: cantaloupe....|       comp.graphics|\n","|38758|Xref: cantaloupe....|       comp.graphics|\n","| 9622|Xref: cantaloupe....|comp.os.ms-window...|\n","| 9911|Xref: cantaloupe....|comp.os.ms-window...|\n","|10094|Path: cantaloupe....|comp.os.ms-window...|\n","| 9943|Path: cantaloupe....|comp.os.ms-window...|\n","|60992|Newsgroups: comp....|comp.sys.ibm.pc.h...|\n","|38750|Path: cantaloupe....|       comp.graphics|\n","| 9485|Xref: cantaloupe....|comp.os.ms-window...|\n","| 9902|Newsgroups: comp....|comp.os.ms-window...|\n","|38867|Newsgroups: comp....|       comp.graphics|\n","| 9758|Xref: cantaloupe....|comp.os.ms-window...|\n","|38921|Newsgroups: comp....|       comp.graphics|\n","|10742|Newsgroups: comp....|comp.os.ms-window...|\n","|38929|Path: cantaloupe....|       comp.graphics|\n","|58994|Path: cantaloupe....|comp.sys.ibm.pc.h...|\n","|60551|Path: cantaloupe....|comp.sys.ibm.pc.h...|\n","|10076|Path: cantaloupe....|comp.os.ms-window...|\n","|38835|Newsgroups: comp....|       comp.graphics|\n","+-----+--------------------+--------------------+\n","only showing top 20 rows\n","\n"]}],"source":["result_list = df[df.topic.like(\"comp%\")].collect()\n","new_df = sc.parallelize(result_list).toDF()\n","new_df.dropDuplicates().show()"]},{"cell_type":"markdown","id":"b2c1dc32","metadata":{"id":"b2c1dc32"},"source":["The output shows the result of the code execution, which includes the top 20 rows of the new DataFrame \"new_df\" after dropping any duplicate rows. Each row represents a record with columns 'id', 'text', and 'topic'. The rows satisfy the condition where the topic starts with \"comp\"."]},{"cell_type":"markdown","source":["### Distinct Group"],"metadata":{"id":"Hh2nv84ZAatl"},"id":"Hh2nv84ZAatl"},{"cell_type":"code","source":["from pyspark.sql.functions import col, regexp_extract\n","\n","# Extract the word followed by a period from the topic column\n","extracted_word = regexp_extract(col(\"topic\"), r\"(\\w+)\\.\", 1)\n","\n","# Filter the DataFrame to select non-null extracted words\n","filtered_df = df.filter(extracted_word != \"\")\n","\n","# Get the distinct topics\n","distinct_topics = filtered_df.select(extracted_word.alias(\"topic\")).distinct()\n","\n","# Show the distinct topics\n","distinct_topics.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EzRCCDHH7Q76","executionInfo":{"status":"ok","timestamp":1686478069209,"user_tz":-60,"elapsed":1539,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"fb75d312-3c18-45fd-d3ba-df893abfc261"},"id":"EzRCCDHH7Q76","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+\n","|topic|\n","+-----+\n","|  alt|\n","|  sci|\n","| misc|\n","|  rec|\n","| comp|\n","|  soc|\n","| talk|\n","+-----+\n","\n"]}]},{"cell_type":"markdown","source":["### COMP group"],"metadata":{"id":"FNImCHy0AMHC"},"id":"FNImCHy0AMHC"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","# Filter the DataFrame to select topics that start with \"comp\"\n","filtered_df = df.filter(col(\"topic\").like(\"comp%\"))\n","\n","# Count the number of topics that start with \"comp\"\n","count = filtered_df.count()\n","\n","print(\"Number of topics starting with 'comp':\", count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dhSt-_p6OYd","executionInfo":{"status":"ok","timestamp":1686477946706,"user_tz":-60,"elapsed":1642,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"0636d865-d2c6-40db-f5ba-be3ba7c3283a"},"id":"_dhSt-_p6OYd","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of topics starting with 'comp': 500\n"]}]},{"cell_type":"markdown","source":["### SCI group"],"metadata":{"id":"yvLIscY6_tKy"},"id":"yvLIscY6_tKy"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","# Filter the DataFrame to select topics that start with \"comp\"\n","filtered_df = df.filter(col(\"topic\").like(\"sci%\"))\n","\n","# Count the number of topics that start with \"comp\"\n","count = filtered_df.count()\n","\n","print(\"Number of topics starting with 'sci':\", count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osHWSDUJ7aP2","executionInfo":{"status":"ok","timestamp":1686478138935,"user_tz":-60,"elapsed":1101,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"b9796402-25fb-4b63-838b-91bd76841fcb"},"id":"osHWSDUJ7aP2","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of topics starting with 'sci': 400\n"]}]},{"cell_type":"markdown","source":["### ALT group"],"metadata":{"id":"q9xGI2mh_xDh"},"id":"q9xGI2mh_xDh"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","# Filter the DataFrame to select topics that start with \"comp\"\n","filtered_df = df.filter(col(\"topic\").like(\"alt%\"))\n","\n","# Count the number of topics that start with \"comp\"\n","count = filtered_df.count()\n","\n","print(\"Number of topics starting with 'alt':\", count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9eAhM307jRE","executionInfo":{"status":"ok","timestamp":1686478154553,"user_tz":-60,"elapsed":1068,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"322905a0-d979-4f74-a3fd-723a2f9f2f6c"},"id":"B9eAhM307jRE","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of topics starting with 'alt': 100\n"]}]},{"cell_type":"markdown","source":["### SOC group"],"metadata":{"id":"HwnW-K-R_2po"},"id":"HwnW-K-R_2po"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","# Filter the DataFrame to select topics that start with \"comp\"\n","filtered_df = df.filter(col(\"topic\").like(\"soc%\"))\n","\n","# Count the number of topics that start with \"comp\"\n","count = filtered_df.count()\n","\n","print(\"Number of topics starting with 'soc':\", count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w6u9_CUW7m9P","executionInfo":{"status":"ok","timestamp":1686478166941,"user_tz":-60,"elapsed":411,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"5f9bf560-70db-458c-d318-cad1f4d82b01"},"id":"w6u9_CUW7m9P","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of topics starting with 'soc': 100\n"]}]},{"cell_type":"markdown","source":["### MISC group"],"metadata":{"id":"p-THk6lL_6f_"},"id":"p-THk6lL_6f_"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","# Filter the DataFrame to select topics that start with \"comp\"\n","filtered_df = df.filter(col(\"topic\").like(\"misc%\"))\n","\n","# Count the number of topics that start with \"comp\"\n","count = filtered_df.count()\n","\n","print(\"Number of topics starting with 'misc':\", count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jb_PWA_57qUv","executionInfo":{"status":"ok","timestamp":1686478191052,"user_tz":-60,"elapsed":999,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"6c73e5a5-32a0-484b-bde7-afcccbea426c"},"id":"Jb_PWA_57qUv","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of topics starting with 'misc': 100\n"]}]},{"cell_type":"markdown","source":["### Talk group"],"metadata":{"id":"FXLNNpDL_-ut"},"id":"FXLNNpDL_-ut"},{"cell_type":"code","source":["from pyspark.sql.functions import col\n","# Filter the DataFrame to select topics that start with \"comp\"\n","filtered_df = df.filter(col(\"topic\").like(\"talk%\"))\n","\n","# Count the number of topics that start with \"comp\"\n","count = filtered_df.count()\n","\n","print(\"Number of topics starting with 'talk':\", count)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFo5Hbhx7yS8","executionInfo":{"status":"ok","timestamp":1686478214546,"user_tz":-60,"elapsed":664,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"ff36decc-c9f9-4f24-a266-eac60599aec7"},"id":"jFo5Hbhx7yS8","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of topics starting with 'talk': 400\n"]}]},{"cell_type":"markdown","source":["We can conclude that topics that start with (comp) are the most relevent so we choose to split the labels to 1 with topic start with comp and 0 else"],"metadata":{"id":"T7PKQfiP-g5h"},"id":"T7PKQfiP-g5h"},{"cell_type":"markdown","id":"6d4df64f","metadata":{"id":"6d4df64f"},"source":["- **adds a new column \"label\" to the DataFrame \"df\" :** "]},{"cell_type":"code","execution_count":null,"id":"c69ec937-3116-4026-b6d6-cd88bed51dd5","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"c69ec937-3116-4026-b6d6-cd88bed51dd5","executionInfo":{"status":"ok","timestamp":1686479055913,"user_tz":-60,"elapsed":956,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"ccb55bd1-1bfd-4b9b-da34-482adbd51078"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------+--------------------+--------------------+-----+\n","|    id|                text|               topic|label|\n","+------+--------------------+--------------------+-----+\n","| 38491|Path: cantaloupe....|       comp.graphics|  1.0|\n","| 10008|Path: cantaloupe....|comp.os.ms-window...|  1.0|\n","| 51539|Path: cantaloupe....|comp.sys.mac.hard...|  1.0|\n","|101603|Xref: cantaloupe....|           rec.autos|  0.0|\n","|102625|Path: cantaloupe....|  rec.sport.baseball|  0.0|\n","+------+--------------------+--------------------+-----+\n","only showing top 5 rows\n","\n"]}],"source":["labeledNewsGroups = df.withColumn(\"label\",df.topic.like(\"comp%\").cast(\"double\"))\n","labeledNewsGroups.sample(False,0.003,10).show(5)"]},{"cell_type":"markdown","id":"442591e0","metadata":{"id":"442591e0"},"source":["The output shows the result of the code execution, which includes a sample of 5 rows from the labeled DataFrame. Each row consists of the columns 'id', 'text', 'topic', and 'label'. The 'label' column contains binary values (0.0 or 1.0) indicating whether the topic starts with \"comp\" or not."]},{"cell_type":"markdown","id":"dc070677","metadata":{"id":"dc070677"},"source":["### Splits the DataFrame :"]},{"cell_type":"code","execution_count":null,"id":"07aeb696-e332-46bc-b1c1-0a6d7ac2fcea","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"07aeb696-e332-46bc-b1c1-0a6d7ac2fcea","executionInfo":{"status":"ok","timestamp":1686479065963,"user_tz":-60,"elapsed":3453,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"004e9615-628b-485d-9e67-4b77fbdf915e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total document count: 2000\n","Training-set count: 1812\n","Test-set count: 188\n"]}],"source":["train_set, test_set = labeledNewsGroups.randomSplit([0.9, 0.1], 12345)\n","print (\"Total document count:\",labeledNewsGroups.count())\n","print (\"Training-set count:\",train_set.count())\n","print (\"Test-set count:\",test_set.count())"]},{"cell_type":"markdown","id":"e8db34e6","metadata":{"id":"e8db34e6"},"source":["The output shows the result of the code execution, which includes the total document count in the labeled dataset, the count of documents in the training set, and the count of documents in the test set."]},{"cell_type":"markdown","id":"a4b61072","metadata":{"id":"a4b61072"},"source":["### The pyspark.ml module for text classification :"]},{"cell_type":"code","execution_count":null,"id":"5b3afad9-edc4-48f0-ba27-c186a723d254","metadata":{"tags":[],"id":"5b3afad9-edc4-48f0-ba27-c186a723d254"},"outputs":[],"source":["from pyspark.ml.linalg import Vector\n","from pyspark.ml import Pipeline, PipelineModel\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","from pyspark.ml.feature import HashingTF,StopWordsRemover,IDF,Tokenizer\n","tokenizer = Tokenizer().setInputCol(\"text\").setOutputCol(\"words\")\n","remover= StopWordsRemover().setInputCol(\"words\").setOutputCol(\"filtered\").setCaseSensitive(False)\n","hashingTF = HashingTF().setNumFeatures(1000).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n","idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n"]},{"cell_type":"markdown","id":"60bab57b","metadata":{"id":"60bab57b"},"source":["### Text Classification Pipeline :"]},{"cell_type":"code","execution_count":null,"id":"f3bdbfbb-2944-41b3-b796-fa25017269c4","metadata":{"tags":[],"id":"f3bdbfbb-2944-41b3-b796-fa25017269c4"},"outputs":[],"source":["from pyspark.ml.classification import NaiveBayes\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","\n","nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n","\n","# Création du pipeline de traitement des données\n","pipeline = Pipeline(stages=[tokenizer,remover,hashingTF,idf, nb])\n"]},{"cell_type":"markdown","id":"a942100c","metadata":{"id":"a942100c"},"source":["The pipeline allows to streamline the data processing and model training steps, making it easier to apply the entire sequence of transformations consistently to both training and test datasets."]},{"cell_type":"markdown","id":"946ab80d","metadata":{"id":"946ab80d"},"source":["### Model fitting :"]},{"cell_type":"markdown","id":"a9ff8471","metadata":{"id":"a9ff8471"},"source":["This step executes the data processing stages defined in the pipeline and trains the Naive Bayes classifier on the processed data."]},{"cell_type":"code","execution_count":null,"id":"305623bb-530f-412a-9c58-aae1ebb19871","metadata":{"tags":[],"id":"305623bb-530f-412a-9c58-aae1ebb19871"},"outputs":[],"source":["model=pipeline.fit(train_set)"]},{"cell_type":"markdown","id":"97a55ed7","metadata":{"id":"97a55ed7"},"source":["### Display a sample of predictions :"]},{"cell_type":"markdown","id":"872b3eb0","metadata":{"id":"872b3eb0"},"source":["this code display a sample of predictions for documents with topics related to \"comp%\". "]},{"cell_type":"code","execution_count":null,"id":"0a08cf97-f883-458d-ac2d-2cf510bd32f1","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"0a08cf97-f883-458d-ac2d-2cf510bd32f1","executionInfo":{"status":"ok","timestamp":1686479163639,"user_tz":-60,"elapsed":1315,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"1dbbee41-dfc9-4ffe-f999-3118716b5a19"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+--------------------+----------+-----+\n","|   id|               topic|         probability|prediction|label|\n","+-----+--------------------+--------------------+----------+-----+\n","|60439|comp.sys.ibm.pc.h...|[0.99999921868087...|       0.0|  1.0|\n","|60698|comp.sys.ibm.pc.h...|[0.99829050858915...|       0.0|  1.0|\n","|61154|comp.sys.ibm.pc.h...|[0.25238237697282...|       1.0|  1.0|\n","|61173|comp.sys.ibm.pc.h...|[1.94623607272862...|       1.0|  1.0|\n","+-----+--------------------+--------------------+----------+-----+\n","\n"]}],"source":["predictions = model.transform(test_set)\n","predictions.select(\"id\",\"topic\",\"probability\",\"prediction\",\"label\").filter(predictions.topic.like(\"comp%\")).sample(False,0.1,10).show()"]},{"cell_type":"markdown","id":"d9a3288e","metadata":{"id":"d9a3288e"},"source":["The displayed output shows the \"id\" of the document, its original \"topic\", the predicted \"probability\" for each class, the final \"prediction\" (class label), and the actual \"label\" from the test set."]},{"cell_type":"markdown","id":"fdaabfa1","metadata":{"id":"fdaabfa1"},"source":["- **includes additional columns in the selected output :**"]},{"cell_type":"code","execution_count":null,"id":"387f39a2-0b81-4b7d-9411-b6f1aa674890","metadata":{"tags":[],"id":"387f39a2-0b81-4b7d-9411-b6f1aa674890","outputId":"81824e47-915a-42bd-cf66-745d7baa35a2"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+\n","|   id|               topic|            filtered|       rawPrediction|            features|         probability|prediction|label|                text|\n","+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+\n","|50439|comp.sys.mac.hard...|[xref:, cantaloup...|[-1680.0068271543...|(1000,[1,2,11,38,...|[0.99690688463568...|       0.0|  1.0|Xref: cantaloupe....|\n","|60766|comp.sys.ibm.pc.h...|[path:, cantaloup...|[-2303.4338113436...|(1000,[2,20,30,38...|[0.99999992702060...|       0.0|  1.0|Path: cantaloupe....|\n","|68239|      comp.windows.x|[newsgroups:, com...|[-921.05984444669...|(1000,[2,8,21,71,...|[0.34788291452040...|       1.0|  1.0|Newsgroups: comp....|\n","+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-----+--------------------+\n","\n"]}],"source":["predictions.select(\"id\",\"topic\",\"filtered\",\"rawPrediction\",\"features\",\"probability\",\"prediction\",\"label\",'text').filter(predictions.topic.like(\"comp%\")).sample(False,00.1,10).show(5)"]},{"cell_type":"markdown","id":"f50445f0","metadata":{"id":"f50445f0"},"source":["The displayed output now includes additional columns such as \"filtered\" (the filtered words from the document), \"rawPrediction\" (the raw prediction values for each class), and \"features\" (the extracted features used for prediction), along with the existing columns from the previous code snippet."]},{"cell_type":"markdown","id":"61f5248e","metadata":{"id":"61f5248e"},"source":["### Performance of the classification model :"]},{"cell_type":"markdown","id":"21c5c87f","metadata":{"id":"21c5c87f"},"source":["By using the evaluator object, we can evaluate the accuracy of the predictions made by the model on the test set."]},{"cell_type":"code","execution_count":null,"id":"e2f36b1c-5676-4d54-88cd-a53f7faae83f","metadata":{"tags":[],"id":"e2f36b1c-5676-4d54-88cd-a53f7faae83f"},"outputs":[],"source":["evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n"]},{"cell_type":"markdown","id":"411ea756","metadata":{"id":"411ea756"},"source":["### Model Accuracy :"]},{"cell_type":"code","execution_count":null,"id":"8021ad33-2d9a-495d-a44d-0dbacf48b76e","metadata":{"tags":[],"colab":{"base_uri":"https://localhost:8080/"},"id":"8021ad33-2d9a-495d-a44d-0dbacf48b76e","executionInfo":{"status":"ok","timestamp":1686479186766,"user_tz":-60,"elapsed":1313,"user":{"displayName":"Doha ben","userId":"15369879026236646516"}},"outputId":"a19c332f-cbb7-4ebb-95c1-2705e0ee7d60"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test set accuracy = 0.8829787234042553\n"]}],"source":["accuracy = evaluator.evaluate(predictions)\n","print(\"Test set accuracy = \" + str(accuracy))"]},{"cell_type":"markdown","id":"c1723bfa","metadata":{"id":"c1723bfa"},"source":["The accuracy of the classification model on the test set is approximately 0.8829, or 88.29%. This means that the model correctly predicted the class labels for about 88.29% of the instances in the test set."]},{"cell_type":"code","execution_count":null,"id":"769acde3-f358-4c9f-858d-a5c0f62c91c9","metadata":{"id":"769acde3-f358-4c9f-858d-a5c0f62c91c9"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}